# History

Here is a translation of the development history of Scientometrics into English.

### **1. Germination of Ideas and Early Practices (Early 20th Century - 1960s)**

This phase was characterized by scattered, groundbreaking ideas and work that laid the foundation for the discipline.

1.  **Rise of Bibliostatistics**:
    *   **1926**: American psychologist **Alfred Lotka** proposed **Lotka's Law**, describing the distribution of author productivity (the number of authors publishing n papers is proportional to 1/n²). This was the first quantitative description of patterns in research output.
    *   **1934**: American linguist **Samuel Bradford** proposed **Bradford's Law**, describing the scattering of literature and revealing the existence of core journal clusters.
    *   **1948**: American scholar **George Zipf** proposed **Zipf's Law** for word frequency distribution in texts. These laws together formed the classical foundation of bibliometrics.

2.  **Beginning of Citation Analysis**:
    *   **1955**: American scientist **Eugene Garfield** published the seminal paper *"Citation Indexes for Science,"* proposing the systematic use of references (citations) to track the development and connections of scientific ideas.
    *   **1960**: Garfield founded the **Institute for Scientific Information (ISI)**.
    *   **1964**: ISI launched the **Science Citation Index (SCI)**, providing the data foundation for large-scale citation analysis—a revolutionary tool innovation.

3.  **Perspective from Sociology of Science**:
    *   Sociologist **Robert K. Merton** and his school emphasized the scientific reward system, priority, Matthew Effect, etc., providing a theoretical basis for understanding the social motivations behind citation behavior.

### **2. Formal Establishment and Rapid Development (1970s - 1990s)**

During this period, the discipline was formally named, research institutions were established, and its theoretical and methodological systems expanded rapidly.

1.  **Naming and Institutionalization**:
    *   **1969**: Soviet information scientist **Vasily Nalimov** and **Zinaida Mulchenko** published the Russian book *"Naukometriya,"* coining the term "Scientometrics" for the first time.
    *   **1978**: The international academic journal **"Scientometrics"** was launched, becoming the flagship journal of the field.
    *   Under the leadership of **Tibor Braun**, the Hungarian Academy of Sciences became one of the world's leading research centers for scientometrics, promoting the discipline's internationalization.

2.  **Development of Core Theories and Indicators**:
    *   **1973**: **Henry Small** introduced the concept of **"Co-citation Analysis,"** using the strength of co-citation between documents to map the structure of science visually.
    *   **1976**: Garfield and others, using SCI data for the first time, drew a macro map of contemporary science through citation analysis.
    *   The **Journal Impact Factor** (proposed by Garfield) gradually evolved from a tool for journal selection into a core indicator for evaluating journal and even article influence.
    *   A series of methods such as **collaboration network analysis, journal co-citation analysis, and author co-citation analysis** were derived.

### **3. The Era of Big Data and Evaluation Applications (2000s - 2010s)**

With digitization and networking, data availability exploded. The applied nature of the discipline, especially its role in research evaluation, became increasingly prominent.

1.  **Data Revolution**:
    *   **2004**: Elsevier launched the **Scopus** database, competing with Web of Science and offering broader journal coverage.
    *   Google launched **Google Scholar**, providing free, wider-coverage academic search and citation data.
    *   The rise of the Open Access movement and institutional repositories created new data sources.

2.  **New Indicators and "Altmetrics"**:
    *   To address the limitations of traditional citation indicators (long time lags, large field differences), new indicators for individual impact like the **h-index** and **g-index** were developed.
    *   After 2010, with the proliferation of social media, **Altmetrics** emerged, focusing on the mention and discussion of papers on platforms like news, blogs, Twitter, and policy documents, attempting to measure their societal impact.

3.  **Central Role in Research Evaluation**:
    *   Scientometric indicators were widely applied in university rankings (e.g., ARWU, THE), institutional assessments, project reviews, and talent recruitment. This sparked significant controversy, leading to widespread debate about **"judging papers by their journal," "indicator misuse,"** and **Goodhart's Law**.

### **4. Current Trends and Future Directions (2020s - Present)**

The discipline is now in a phase of reflection, integration, and technological innovation.

1.  **Responsible Metrics and Ethical Reflection**:
    *   In response to the **Leiden Manifesto (2015)** and the **Declaration on Research Assessment (DORA, 2012/2022)**, there is a strong emphasis on **"Responsible Research Assessment."** This advocates for **opposing single metrics, combining peer review, considering disciplinary differences, and evaluating substantive contributions**.
    *   Increasing attention is paid to biases in indicator use (e.g., language, nationality, gender, disciplinary biases).

2.  **Technology-Driven and Multi-Source Data Integration**:
    *   Widespread adoption of advanced technologies like **machine learning, natural language processing, and complex network analysis**.
    *   Data sources are no longer limited to papers and citations but integrate **patents, clinical trial data, government reports, commercial product information, funding data**, etc., to build a more comprehensive framework for assessing research impact.

3.  **Open Science and Reproducibility**:
    *   The proliferation of open access, open data, and open code provides richer and more transparent objects for metric studies.
    *   Scientometrics research itself is also beginning to emphasize the reproducibility of methods and data.

4.  **Macro Strategy and Policy Support**:
    *   Serving national-level S&T strategies—identifying emerging frontiers ("tipping point" detection), evaluating research collaboration networks, monitoring technological competitiveness—has become a crucial智库 tool for national innovation policy.

### **Summary**

The history of Scientometrics is a journey from **describing patterns** to **mapping structures**, then to **assessing impact**, and finally moving toward **responsibly supporting science governance**. It has always been accompanied by **innovations in data tools** (from manual statistics to SCI, to big data platforms) and the **expansion of application scenarios** (from library management to global research evaluation). The core challenge for the future is how to leverage powerful quantitative tools while adhering to the essence of science—**promoting innovation, reliability, and socially meaningful contribution**—and avoiding the pitfall of "research for metrics."


Excellent. Building upon the overview of scientometrics, here is a systematic outline of the development of **bibliometrics**. Bibliometrics is often considered the parent field, specifically focusing on the quantitative analysis of written publications and their communication processes.

### **The Development of Bibliometrics**

Bibliometrics is the **quantitative study of publications, their authors, and the patterns of communication within a given field or body of literature**. Its evolution is closely intertwined with, and often precedes, that of scientometrics.

---

### **Phase 1: Foundational Laws and Empirical Beginnings (Pre-1950s)**
This era established the core statistical laws that form the bedrock of the discipline.

*   **1922 - Lotka's Law of Scientific Productivity:** Alfred J. Lotka's analysis of chemistry publications revealed an inverse square relationship, showing that a small number of authors produce a large proportion of the literature. This introduced the concept of quantifying author productivity.
*   **1934 - Bradford's Law of Scattering:** Samuel C. Bradford observed that the literature of a scientific discipline is concentrated in a small core of journals, with the remainder scattered across a much larger "tail." This law is fundamental to collection management in libraries.
*   **1948 - Zipf's Law of Word Frequency:** George K. Zipf's principle states that in a corpus of natural language, the frequency of any word is inversely proportional to its rank in a frequency table. This laid the groundwork for text analysis and keyword studies.
*   **Context:** These laws emerged from empirical observation in specific fields (chemistry, geophysics, linguistics). The primary goal was to understand patterns for practical library and information management purposes.

### **Phase 2: Formalization, Citation Analysis, and Institutionalization (1950s-1970s)**
The field gained its name, a powerful new tool (citation indexing), and formal academic recognition.

*   **1969 - Coining of the Term "Bibliometrics":** Although quantitative studies existed for decades, the term **"bibliometrics"** was officially proposed by Alan Pritchard to replace the earlier "statistical bibliography," defining it as "the application of mathematics and statistical methods to books and other media of communication."
*   **The Citation Revolution (1955 onwards):** Eugene Garfield's 1955 paper and the subsequent creation of the **Science Citation Index (SCI)** in 1964 were transformative. This enabled:
    *   **Impact Measurement:** Tracing who cites whom allowed for the measurement of a document's or journal's influence, leading to the **Journal Impact Factor (JIF)**.
    *   **Mapping Science:** The analysis of citation networks (co-citation, bibliographic coupling) allowed researchers to visualize the intellectual structure of scientific fields.
*   **Establishment as an Academic Discipline:** University courses, dedicated research groups (e.g., in Belgium and the UK), and specialized journals (like the *Journal of the American Society for Information Science*) began to appear, solidifying its academic standing.

### **Phase 3: Maturation, Diversification, and the Rise of Performance Evaluation (1980s-1990s)**
Bibliometric methods became more sophisticated and began to be used explicitly for assessment.

*   **Beyond the Core Laws:** Research expanded into author collaboration networks, journal-to-journal citation maps, and the dynamics of literature growth and obsolescence.
*   **From Description to Evaluation:** Bibliometric indicators (publication counts, citation counts, JIF) were increasingly adopted by policymakers and administrators to evaluate the research performance of **individuals, research groups, institutions, and even nations**. This marked a significant shift from a descriptive to an evaluative science.
*   **Software Development:** The advent of specialized software facilitated the analysis of large datasets from citation indexes, making complex network analyses more accessible.

### **Phase 4: The Digital & Networked Era: Expansion and Critique (2000s-2010s)**
The internet and digital publishing revolutionized data sources and scale, but also triggered major critiques.

*   **Data Explosion:** The rise of **Scopus (2004)** and **Google Scholar** provided broader, alternative data sources to the Web of Science. Full-text digital archives became available for analysis.
*   **New Indicators:** The **h-index (2005)** was introduced by Jorge E. Hirsch as a composite measure of an author's productivity and citation impact, quickly becoming a standard metric.
*   **Intensified Critique and the "Metrics Tide":**
    *   The widespread, often uncritical, use of bibliometrics for evaluation (e.g., hiring, promotion, funding) led to backlash. Critics highlighted:
        *   Field differences in citation practices.
        *   The limitation of citations as a proxy for quality or societal impact.
        *   The phenomenon of **"Goodhart's Law"** – when a measure becomes a target, it ceases to be a good measure.
    *   The **"Impact Factor Game"** was criticized for distorting publishing behavior.

### **Phase 5: The Contemporary Era: Responsible Metrics, Open Science, and Computational Power (2010s-Present)**
The field is now characterized by self-reflection, ethical guidelines, and new technological frontiers.

*   **The Responsible Metrics Movement:** In response to critique, major initiatives emerged:
    *   **The Leiden Manifesto (2015):** Offered ten principles for the responsible use of research metrics.
    *   **The San Francisco Declaration on Research Assessment (DORA, 2012):** Specifically advocated moving away from journal-based metrics (like JIF) for assessing individual scientists.
*   **Integration of New Data Sources:**
    *   **Altmetrics:** Captures online attention (news, blogs, social media, policy documents) to gauge broader, societal impact.
    *   **Open Science Indicators:** Tracks the use of open datasets, software, and protocols.
*   **Technological Advances:** The application of **machine learning, natural language processing (NLP), and large-scale network analysis** allows for thematic mapping, trend prediction, and the analysis of full-text content at scale.
*   **Focus on Openness and Reproducibility:** There is a growing emphasis on open bibliometric data, transparent methodologies, and reproducible research within the field itself.

### **Conclusion: From Library Shelves to Global Evaluation**
The development of bibliometrics reflects a journey:
1.  **From** empirical **laws** describing publication patterns for practical library science.
2.  **Through** the **institutionalization** of citation analysis as a tool for understanding science.
3.  **To** its controversial but dominant role in **research evaluation** and management.
4.  **And now towards** a more **responsible, sophisticated, and multi-dimensional** discipline that leverages computational power to understand scholarly communication while being acutely aware of the ethical implications of its own indicators.


