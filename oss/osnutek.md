# Informetrična podpora odprte znanosti in njena analiza
**V pripravi !!!**

Zaradi eksponentne rasti števila znanstvenih del tradicionalni načini znanstvenih objav ne ustrezajo več potrebam v več pogledih.
Po eni strani so potrebna orodja, ki iz množice objav izberejo tiste, ki naj zanimajo. Po drugi strani pa
se v razmišljanjih o znanstvenih objavah preveč naslanjamo na okvire, ki jih je postavil tisk. Ko objava zaživi na spletu, omogoča neposredne povezave na druge objave oz. podatke. Po drugi strani pa so lahko popravki in dopolnila časovno dinamični (Včasih je bil potreben poseben prispevek "Errata"). Seveda pa take novosti prinašajo nove izzive. Tudi sicer se je sistem znanstvenih objav izrodil. Zato kliče po temeljiti in dobro premišljeni prenovi [CSsp].


## Razvoj hranjenja in posredovanja znanja

Spočetka so ćloveške skupnosti nabirale in prenašale znanje iz roda v rod. Izuma risbe in na njem osnovane pisave sta omogočila "skladiščenje" znanja in njegovo razširjanje v prostoru in času. Evklidovi Elementi predstavljajo prvi znani poizkus povzetja in na logiki temelječe ureditve nakopičenega geometrijskega znanja. Večina znanja sredozemskih in bližnjevzhodnih ljudstev  je bila zbrana v Aleksandrijski knjižnici.

Po propadu Rimskega cesarstva je v Evropi krščanstvo "cenzuriralo" antično  zapuščino. Dobršen del so ga ohranili Arabci in oplemenitili z znanjem Indije in osrednje Azije. Žal so marsikaj od zbranega uničili Mongoli ob zasedbi vzhodnih arabskih dežel (Bagdad).

Velik problem pri dostopnosti in obstojnosti nakopičenega znanja je bilo počasno in drago razmnoževanje - prepisi. Zato je zelo pomemben korak v razvoju hranjenja in razširjanja znanja izum tiska (Gutenberg, ~1450). Ta je močno pohitril in pocenil razmnoževanje zapisov in s tem omogočil veliko širšo dostopnost in večjo obstojnost znanja. Začeli so tiskati knjige in druge oblike zapisov (listki, časopisi, itd.). Pojavila so se znanstvena društva, ki so začela izdajati svoje revije (franc, ang). 

V obdobju razsvetljenstva so v Franciji začeli izdajati Enciklopedijo, ki naj bi povzela vse človeško znanje in vedenje. Razširile so se tudi knjižnice in naraščalo je število del. Sčasoma je število del preraslo pomnilne zmožnosti  knjižničarjev. Za pregled v knjižnici zbranih del so pripravili seznam del -  pogosto v knjižni obliki. Težava te rešitve je bil vnos sprememb. Te so dopisovali na ustrezna mesta. 

Kot ustreznejšo rešitev so začeli uporabljati  kartične kataloge (avtorski, stvarni) - zbirke kartic. Vsaka kartica je  vsebovala podatke o posamičnem delu. Kartice so bile gleda na izbrano urejenost (za hitro iskanje) shranjene v predalčke. Višek tega pristopa predstavlja katalog Mundaneum (Otlet & La Fontaine, Bruselj 1895-1939), ki je vseboval 18 milijonov kartic v 15 tisoč predalčkih s podatki o večini do takrat objavljenih del [WPmund].

V svojem članku "As We May Think" (1945) je Vannevar Bush nakazal rešitev 
> “Consider a future device …  in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory.” [AWMT] 

ki se je začela uresničevati v osemdestih letih z osebnimi računalniki.
 
Razvoj računalnikov se je začel pred drugo svetovno vojno (Zuse). V splošno rabo pa so začeli vstopati v 50. letih. Sprva so bili velike naprave nameščene v računalniških centrih. Njihova zmogljivost je hitro naraščala. V računalnikih so podatki shranjeni v digitalni obliki (zaporedja 0 in 1). Računalniki so združili medije (števila, besedila, (gibljive) slike, zvok, programe, itd.) v eni napravi, omogočili hranjenje in obdelavo velikih količin podatkov. Razmnoževanje podatkov v digitalni obliki je enostavno in poceni.
 

Scientometrija je podpodročje informetrike, ki proučuje kvantitativne vidike znanstvene literature. Medtem ko se je sociologija znanosti osredotočala na vedenje znanstvenikov, se je scientometrija osredotočala na analizo publikacij.
Sodobna scientometrija večinoma temelji na delu Dereka J. de Solla Pricea (Science since Babylon, 1961; Little Science, Big Science, 1963) in Eugena Garfielda. Slednji je ustanovil Inštitut za znanstvene informacije (Institute for Scientific Information (ISI), 1960; sedaj  Clarivate Analytics) in ustvaril Science Citation Index, ki je pogosto vir podatkov za scientometrične analize. Posebna akademska revija Scientometrics je bila ustanovljena leta 1978.
Mednarodno združenje za scientometrijo in informetrijo ISSI je bilo ustanovljeno leta 1993 [WPim, WPsp].
Pretirano zanašanje na scientometrijo pri vrednotenju znanstvenih del je podlaga za načelo "objavljaj ali izgini" ("publish or perish"), ki vodi v raziskave nizke kakovosti. To načelo je ena izmed pojavnih oblik Goodhartovega zakona: Ko mera postane cilj, preneha biti dobra mera [PBP].


Informetrija je preučevanje količinskih vidikov informacij. Je razširitev in izpopolnitev tradicionalne bibliometrije in scientometrije. Informetrija uporablja bibliometrične in scientometrične metode za preučevanje predvsem problemov upravljanja informacij o virih ter vrednotenja znanosti in tehnologije [WPim]. Obstaja še več drugih sorodnih področij, ki upoštevajo tudi drugi medije in oblike znanstvenih gradiv (zapiski, poročila, načrti, patenti, primerki, podatki, programi, posnetki, itd.) [iMet]. Odnose med njimi je na spodnji sliki povzel Alexander Doria

<img src="https://github.com/user-attachments/assets/346a2f13-1eb1-407e-b28b-be8abd558167" width="450" />

Računalnike lahko med seboj tudi podatkovno povežemo. Razvoj povezljivosti se je začel že leta 1970 (ARPANET) v širšo akademsko rabo pa je vstopil v drugi polovice osemdesetih let (MAIL, FTP, Telnet). Splošna povezanost računalnikov se je razmahnila v devetdesetih letih  v obliki spleta (World Wide Web (WWW), Tim Berners-Lee , 1990).

Objava članka v reviji zahteva svoj čas - od nekaj tednov do nekaj let. Za pohitritev  raziskav so v drugi polovici prejšnjega stoletja avtorji pogosto članek natisnili v obliki poročila ali predobjave (preprint) in ga poslali svojim kolegom. Paul Ginsparg je avgusta 1991 ustvaril shrambo (repozitorij) za predobjave, poimenivano arXiv, na računalniku v Los Alamos National Laboratory (LANL), ki je bil dosegljiv z računalnikov vključenih v splet. arXiv je odprto dostopna shramba za predobjave ali poobjave sprejetih (nadzor brez recenzije) člankov v digitalni obliki (e-prints). Sprejema članke iz  matematike, fizike, astronomije, elektrotehnike, računalništva, kvantitativne biologije, statistike, matematičnih financ in ekonomije. Posnema ga več podobnih shramb: TechRxiv, ChemRxiv, bioRχiv, medRχiv, PsyArXiv, SocArXiv, HAL, RUL, zenodo, itd. Pojavile so se shrambe tudi za druge oblike del: programje (c|Net, CPAN, CTAN, CRAN, PyPI (Python), SourceForge), članki (CiteSeerX, ResearchGate, Google Scholar, Academia, DBLP), knjige (Project Gutenberg, Open Library), podatki (Kaggle, Microsoft Research Open Data, Google Dataset Search,  UCI Machine Learning Repository, CERN Open Data, Dryad, Harvard Dataverse, PEW research, European Social Survey), mediji (YouTube, Flickr, Google photos, Slideshare, NASA), projekti (GitHub, Figshare, Zenodo).

Splet (WWW) sestavljajo strani z vsebino, ki so lahko med seboj povezane. Vsaka stran ima svoj naslov po katerem lahko dosežemo njeno vsebino. Spočetka je uporaba spleta temeljila na kazalih - straneh z zbirkami naslovov zanimivih strani (ponavadi na izbrano temo). ???primer Kmalu rešitev s kazali ni več dohajala (pokritost) hitre rasti spleta. Nadomestili so jo zbiralni programi, ki pregledujejo splet in dodajajo v svoj seznam še neznane strani. Na uporabnikovo zahtevo pa iz tega seznama vrnejo naslove nekaj zahtevi najbližjih strani (Lycos, Google). Pri izboljšavi teh iskalnikov sta imeli pomembno vlogo reklame ("brezplačnost" storitve) in zbiranje podatkov o uporabniku (odgovori in reklame glede na uporabnikova zanimanja).
Web 2.0 je omogočil ustvarjanje spletnih vsebin tudi običajnim, računalniško nepodkovanim uporabnikom. Vseveč strani ni stalnih temveč se sproti ustvarijo na uporabnikovo zahtevo. [SemW]

Za reševanje problemov na računalniku potrebujemo ustrezna orodja - programe. Večinoma jih moramo kupiti ali pa sami napisati. Konec šestdestih let je 	Niklaus Wirth razvil programski jezik Pascal. Kodo pascalskega prevajalnika je bilo mogoče dobiti brezplačno. To je eden zgodnejših primerov proste in odprte programske opreme. Odprtost je za uporabnika zelo pomembna, ker lahko (načeloma) preveri kaj program počne in tudi izve kako (učenje). Po potrebi lahko tak program tudi prilagodi svojim posebnim potrebam (jezik vmesnika, dodatne zmožnosti, itd.). Pojavilo se je več tovrstnih programov, kar se je v osemdesetih letih prelevilo v gibanje za prosto programsko opremo GNU in formaliziralo z Manifestom GNU (Richard Stallman, 1985). Za hiter razvoj znanosti je zelo pomemben prost dostop do zbirk (knjižnic, paketov) (pod)programov in tudi podatkov - raziskovalcem ni potrebno začenjati znova, temveč stvari le dograjujejo. Proti koncu devetdesetih se je pojavilo sorodno odprto-kodno gibanje, ki vključuje tudi možnost trženja programov. Pri ameriških raziskovalnih projektih (NSF) je večkrat uveljavljeno načelo "kar je bilo razvito z javnimi sredstvi, naj bo tudi javno dostopno". To omogoča hiter prenos znanstvenih rezultatov v uporabo. Lep primer tega je razvoj prvega uspešnega slikovnega spletnega pregledovalnika Mosaic, ki je bil osnova tako za Netscape kot tudi za Microsoftov Internet Explorer.

Prav tako je na osnovi raziskovalnega projekta nastal Google. Google je pokazal, da z ustrezno poizvedbo lahko hitro pridemo do ustreznih vsebin strani, ki jih je zbral na spletu. Odgovor dobimo v obliki seznama zadetkov - spletnih naslovov strani, ki ustrezajo poizvedbi. V zadnjem času nam z uporabo umetne inteligence ta seznam nadomesti s povzetkom, ki ga uporabnik lahko v pogovoru s storitvijo izpopolni.

Amazon je pokazal, da z opisi - podatki o posameznih rečeh iz dejanskega sveta lahko te reči vključimo (naredimo dostopne) v splet. Spletno rešitev lahko uprabimo tudi za organizacijo znanstvenih objav. Pri tem je spletni okvir presplošen. Smiselno se je omejiti na znanstvene objave. Za dostopnost posameznega dela je potrebno, da je vključeno v zbirko storitve. To dosežemo z digitalizacijo del, ki ima dve osnovni obliki:
- polna: delo je že ustvarjeno v digitalni obliki ali je vanjo pretvorjeno, dodan je opis dela
- delna: za delo ustvarimo le njegov opis, ki vsebuje tudi podatke o dostopu do dela samega (avtorske pravice)
Za razvoj digitalizacije zgodovinskih in kulturnih virov skrbi združenje TEI (Text Encoding Initiative). Za pripravo spletnih opisov del pa skrbi Dublin Core Metadata Initiative (DCMI). Na področju knjižničarstva pa imamo BIBFRAME: Bibliographic Framework Initiative.

Opisi del vsebujejo različne enote (avtor, delo, vir, ustanova, država, itd.). Zaplete se pri identifikaciji teh enot pri sestavljanju opisa. Podatki so namreč lahko dvoumni (različne enote imajo lahko isto ime (niz znakov)) ali se pojavljajo sopomenke (isti enoti pripada več imen). Najbolje je problem identifikacije reševati pri vnosu podatkov v zbirko in za enote oporabljati enolične identifikatorje (ORCID, DOI, ISSN, ISBN, ISO alpha 2, URL, URI, itd.). Tako dobimo kakovostne podatke za informetrične analize. Stvar bi lahko precej olajšali sami avtorji del, če bi uporabili te enolične identifikatorje že pri pripravi del.


Od izuma tiska sta bili glavni obliki znanstvenih objav članek in knjiga. Informacijska tehnologija (IT) je močno vplivala tudi na znanstvene objave.
- ponudila je nove možnosti komuniciranja med znanstveniki (elektronska pošta, prenos datotek, podpora sodelovanju - skupna področja, na katerih uporabniki lahko objavljajo, popravljajo, komentirajo, itd.)
- digitalna dela je enostavno podvojiti (težave z avtorskimi pravicami).
- programska orodja za ustvarjanje in oblikovanje besedil (TeX, Word itd.) omogočajo avtorju, da delo v celoti pripravi za objavo.
- dela v digitalni obliki lahko presežejo tiskarske zmožnosti (večpredstavnost, koda, notranje in zunanje povezave),
- problem trajnosti.
  
Ena od pomembnih značilnosti tiskanih del je njihova "zaključenost". IT jo v veliki meri rahlja. Digitalni članek lahko popravljamo, zbiramo mnenja (vsebinske pripombe, ocene) o njem, sklice nanj, število dosegov, itd. Vanj lahko, na primer, vključimo 3 razsežne prikaze [X3D], ali zelo podrobne slike v vektorskem opisu [SVG], ali pa videoposnetek, ki si jih lahko ogledujemo z ustreznim prikazovalnikom. V zemljepisni učbenik bi lahko vgradili tabele, ki vsebujejo tekoče podatke o neki zemljepisni enoti.

Iz arheologije poznamo zapise vklesane v kamen, vtisnjene v glino, vrezane v les, zapisane na papirus, pergament ali papir, itd., ki so se v celoti ali okrnjeni ohranili skozi več stoletij ali celo tisočletij in preživeli civilizacije, ki so jih ustvarile. Z digitalnimi zapisi pa je težava - večina nosilcev zapisa (trakovi, diskete, diski, CD, DVD, itd.) ima zagotovljeno trajnost okrog 5 let [DurSM]. Izjema so plošče M-disc, ki naj bi vzdržale vsaj 1000 let [Mdisc]. Dodatna težava je dostop do starejših bralnih/pisalnih naprav. Za zagotavljanje obstojnosti zapisov jih je priporočljivo hraniti v več izvodih na različnih nosilcih in občasno prepisati na novejše nosilce. Zapisi so pripravljeni v različnih oblikah (formatih). Tudi oblike zapisov se spreminjajo. Za hrambo se je, če se le da, smiselno nasloniti na znakovne datoteke in uporabiti oblike zapisov, ki ohranjajo zgradbo podatkov (TeX, SGML, XML, JSON, itd.).
 

Graf znanja je formalizem za predstavitev podatkov in znanja v obliki grafa. Formalno je graf znanja definiran kot označen usmerjen graf sestavljen iz množice trojic oblike (Subjekt,Lastnost,Objekt). Subjekt in Objekt predstavljata vozlišča grafa in trojice predstavljajo imenovane povezave grafa, ki definirajo relacije med vozlišči.

Jezik za predstavitev grafov znanja RDF (Resource Description Framework) je bil predlagan v okviru razvoja Semantičnega spleta (Web 3.0). RDF omogoča enostavno predstavitev grafov s tekstovno datoteko. Vsaka vrstica datoteke vsebuje eno trojico. Na primer, vrstica "Cankar Kraj-rojstva Vrhnika." je izjava, ki pravi, da se je Cankar rodil v Vrhniki. Imeni Cankar in Vrhnika predstavljata vozlišča in celotna trojica predstavlja povezavo z oznako Kraj-rojstva.

V zadnjem desetletju so grafi znanja en izmed najbolj uporabljenih medijev za predstavitev in izmenjavo podatkov in znanja v znanosti in industriji. V znanosti je RDF pogosto uporabljen za izmenjavo podatkov, kot so npr. rezultati eksperimentov. V računalništvu so grafi znanja medij za predstavitev znanja iz področja delovanja danega informacijskega sistema. Na primer, za potrebe pri izvajanju svojih informacijskih sistemov razvijajo in vzdržujejo grafe znanja vsa večja podjetja, kot so npr. Google, Microsoft, Amazon, LinkedIn, Facebook in druga.

??? keywords localication, distinction

## Stanje

Zaradi hitre (trenutno še eksponentne) rasti števila novih del postaja dosedanji pristop k objavljanju neustrezen (rast števila revij, megarevije, plenilske revije, problem pridobivanja recenzentov, goljufije ipd.).

<img src="https://github.com/user-attachments/assets/cf0a4851-b251-4117-bd35-41daadf4aaf3" width="500" />

Glede na OpenAlex se je število objavljenih znanstvenih del podvojilo v 19 letih med 1971 (881943 del) in 1989 (1847109 del), nato v naslednjih 12 letih 
2001 (3705036 del), nato že v 8 letih 2009 (7275504). Leta 2020 je bilo objavljenih 11017156 del. Podatki za zadnja leta so najbrž nepopolni. 

Seveda rast ni neomejena. Če privzamemo privzdignjeni logistični model (v začetnem delu je rast skoraj eksponentna)

f(x;L0,L1,x0,k) = L0 + L1/(1+exp(-k*(x-x0))), &nbsp;&nbsp; L = L0 + L1

dobimo kot lokalna ekstrema naslednji priležnici

modra:  L0 = 914793.1,  L1 = 12310320,   k = 0.1323116,  x0 = 2009,   L = 13225113 <br />
rdeča:&nbsp;&nbsp;  L0 = 798003.8,  L1 = 18594720,   k = 0.1064136,  x0 = 2016,   L = 19392724

ki se precej dobro prilegata podatkom, a zaradi pomanjkljivih podatkov za zadnja leta, dajeta zelo različni napovedi.

Iz podatkov Scopus med letoma 1996 in 2011 izhaja, da je v tem obdobju objavilo vsaj  en članek 15 milijonov znanstvenikov, a le 150.608, manj kot 1%, vsako leto. Njihova imena so v 41% vseh člankov in med soavtorji 87% najbolj citiranih člankov.



Anomalije in goljufije  [WPsm]
1. članki z zelo veliko soavtorji. Članek "COVIDSurg Collaborative and GlobalSurg Collaborative: Timing of surgery following SARS-CoV-2 infection: an international prospective cohort study. Anaesthesia 2021, 76, 748–758" ima 16162 soavtorjev.
2. avtorji, ki so v prvih desetih mesecih v letu 2024 bili soavtorji zelo veliko člankov: Wiwanitkit, V. (492), Daungsupawong, H. (346), Bruze, M. (336), etc.
3. izrojenost recenziranja [PeerA, PeerP].
4. dvig pomembnosti ustanov s "kupovanjem" vrhunskih znanstvenikov [ElPais].
5. plenilske revije in konference  [Beall]: revije z več tisoč letnimi objavami (leta 2024: Scientific Reports (27528), Heliyon (16271), PLoS ONE (14912), IEEE Access (11726), itd.), tematski zvezki revij [WPmj].
6. zahteve recenzentov ali urednikov, da se v članek vključi sklice na nepotrebne vire [Eref].
7. avtorske goljufije (plagiati, kupljeni članki, ChatGPT, izmišljeni/prirejeni podatki, itd.) [OAfraud].

Goodhart (grob osnutek) 
- Obstajajo indikacije, da pri informetričnem spremljanju objav prihaja do zamenjave ciljev: namesto kvalitete se nagrajujejo drugi kvantitativni parametri: npr. odmevnost objav ali celo odmevonst revij, v katerih se članki objavljajo. Uporabniki se temu prilagajajo.
- Zaradi tega se množijo založbe in revije, ki poskušajo na vse načine povečati popularnost objav, ne glede na kvaliteto. Za objavo v predatorskih revijah je pomembno imeti dostop do javnega denarja.
- Prihaja do odtekanja javnega denarja v privatne žepe založniških multinacionalnih hiš.


Odprta znanost je sodoben pristop znanstveno-raziskovalnega dela in razširjanja njegovih rezultatov (znanja, vedenja) na pregleden in sodelovalen način. Odpiranje znanosti preko čim hitrejše izmenjave informacij in znanja med raziskovalci povečuje možnosti za hitrejši napredek v znanosti [aris]. [Ols, Shadow, fAP, ISC, peerE, raj, EU]

Leta 1942 je Robert K. Merton v članku A note on science and democracy (ponatisnjenem v knjigi The Sociology of Science) zapisal štiri "stebre" znanosti ("komunizem", univerzalnost, delo za skupno dobro (nesebičnost, etičnost), skepticizem). Kasneje je bila dodana še izvirnost [WPmn]. Nanje se sklicuje tudi Chubin, D. E. v svojem članku Open science and closed science: Tradeoffs in a democracy (1985) [OsCs]. V dobi spleta je gibanje za odprto znanost postavilo kot glavni steber prost dostop do znanstvenih objav, kar se odraža v deklaraciji konference v Budimpešti leta 2001. Pravno to urejajo licence  Creative Commons ustvarjene leta 2002. V nadaljevanju se je zahteva po odprtosti razširila še na podatke in programje. Odprtost je mogoče izvesti s spletnimi repozitoriji.

Za informetrične analize je pomemben prost dostop do opisov (metapodatkov) objavljenih znanstvenih del - tudi tistih objavljenih v naročniških revijah. Prizadevajo si, da bi prost dostop veljal tudi za povzetke [I4OA]. Pomemben korak v tej smeri je OpenAlex.

OpenAlex je popolnoma odprt katalog globalnega raziskovalnega sistema. Poimenovan je po starodavni Aleksandrijski knjižnici. Vzpostavila ga je neprofitna organizacija OurResearch. Dostopen je postal januarja 2022 preko uporabniškega vmesnika, brezplačnega API-ja ali posnetka vseh podatkov, ki ga lahko prenesemo na svoj računalnik. Velja za nadomestek storitve Microsoft Academic Graph, ki je bila ukinjena 31. decembra 2021. OpenAlex temelji na 7 vrstah enot (entitet): delo W, avtor A, vir S, ustanova I, koncept C, založnik P ali financer F. Rešuje nekaj pomembnih vprašanj za analizo bibliografskih podatkov:

1. identifikacija bibliografskih enot (ID, razločevanje)
2. brezplačen dostop (skupna raba pridobljenih podatkov, prenos na uporabnikov računalnik)
3. izboljšanje vsebine s sodelovanjem uporabnikov (uporabnik odda zahtevo za popravek)

OpenAlex lahko uporabljamo preko spletnega uporabniškega vmesnika ali pa programsko s klici API. Za zahtevnejše obdelave je težava omejitev na največ sto tisoč klicev dnevno. Tej omejitvi se lahko izognemo z vzpostavitvijo kopije OpenAlexa na lokalnem računalniku.

OpenAlex vsebuje podatke o večini del, ki jih najdemo v tržnih storitvah Web of Science in Scopus. Poleg tega pa še vrsto drugih del - na primer podatke o objavah v shrambi arXiv. To nam omogoča, da spremljamo, kaj so trenutne "vroče" teme v znanosti. Podatki iz tržnih storitev so omejeni na uveljavljene revije in vsebujejo časovni zamik povzročen s postopkom objave člankov.

Za uporabo podatkov iz OpenAlexa je potrebno razviti ustrezno programsko podporo, ki zbere in pretvori želene podatke v obliko primerno za podatkovne (statistične, omrežne, AI) analize. Posebej pomembna je podpora čiščenju podatkov in preverjanju kakovosti. Včasih je potrebno podatke tudi izpopolniti - na primer določiti kategorijo sklicev (agreement, comparison, defnition, diference, disagreement, hypothesis, method, position, result, similarity) [CiteC].


## Projekt

Znanstveno objavljanje -
Pričakovanja avtorja-znanstvenika
1. takojšnja objava in dostop,
2. ”registracija” – časovni žig,
3. enostavno najti,
4. neomejen dostop,
5. trajna hramba (dolgoživost, trajnost),
6. odziv bralcev (skupnosti), priznanje.
   
To je mogoče doseči s shrambami del.
Vrednotenje dela ni glavna avtorjeva skrb – pomembno je za njegove/njene delodajalce, financerje itd.

Vrednotenje znanstvenih rezultatov daje prevelik poudarek številu člankov in številu citatov člankov. Obstajajo še druge oblike del: knjige, patenti, načrti, filmi, podatki, programi, spletne strani, itd. Pred kratkim je na spletnem seminarju znani statistik John Bailer dejal: kaj je mojih nekaj tisoč citatov v primerjavi z milijoni, ki uporabljajo Wickhamov Tidyverse. Obstaja več gibanj in priporočil, ki poskušajo omogočiti in uveljaviti celovitejše vrednotenje znanstveno raziskovalnega dela, kot so ORCID, CRediT (Contributor Roles Taxonomy), Leiden Manifesto for Research Metrics, San Francisco Declaration on Research Assessment (DORA), The Vancouver Recommendations, Coalition for Advancing Research Assessment (CoARA). Pomembna postaja možnost ponovitev poskusov in analiz, za kar si prizadeva gibanje FAIR (Findability, Accessibility, Interoperability, and Reuse) [FAIR, goFAIR].

Obstajajo indikacije, da pri informetričnem spremljanju objav prihaja do zamenjave ciljev: namesto kvalitete se nagrajujejo drugi kvantitativni parametri: npr. odmevnost objav ali celo odmevnost revij, v katerih se članki objavljajo. Uporabniki se temu prilagajajo. Zato se množijo založbe in revije, ki poskušajo na vse načine povečati popularnost objav, ne glede na kvaliteto. Za objavo v plenilskih revijah je pomembno imeti dostop do javnega denarja. Prihaja do odtekanja javnega denarja v privatne žepe založniških multinacionalnih hiš. Radikalno se menjajo modeli objavljanja od naročniškega do odprtodostopnih. Prevladujejo objave z APC (Article Processing Charge) in hibridni model objavljanja.

Kot odgovor na te izzive so se pojavili novi modeli objavljanja, kot je »objavi, preglej, izboljšaj« (PRC - publish, review, curate). Ta model obrne tradicionalni pristop pregleda in nato objave, tako da delo najprej objavi na spletu in ga nato podvrže strokovnemu pregledu. Namen tega pristopa je povečati preglednost in pospešiti razširjanje raziskav [PRC].

Izdelava grafa znanja za podatkovno okolje OpenAlex. Graf znanja OpenAlex bo vseboval ontologijo, ki vključuje vrste člankov, znanstvena področja, instititucije, ter druge entitete povezane s članki. Konstrukcija grafa znanja OpenAlex je možna s povezovanjem na obstoječe grafe znanja, kot so na primer DBpedia, WikiData, Yago in drugi. Ti grafi znanja v večini predstavljajo splošno znanje o svetu (angl. common-sense knowledge).

Novejši sistemi za analizo velikih podatkovnih zbirk, kot je npr. odprto-kodni sistem Apache Spark, omogočajo implementacijo kompleksnih analiz na podatkih, ki so bodisi v tekstovni obliki (JSON) ali v grafovski obliki. Analize se lahko izvajajo na osnovi interaktivnih poizvedb v SQL ali na osnovi programov v splošnih programskih jezikih kot so Java, Python in R.

### Cilji

Tematski sklopi
  - OpenAlex (lokalni, razvoj podpore, povezljivost s slovenskimi bazami, študij grafovskih baz, metode uporabe)
  - odprto objavljanje (PRC, študij, spremljanje in vključevanje v dogajanja, AMC podpora urednikom, etična vprašanja)
  - podpora odprti znanosti (informetrične analize, teoretična in programska podpora)
  - razvoj in programska podpora metod za bibliometrične analize in storitve

Razdeliti po sklopih 
  - lokalni OpenAlex; izdelava programske podpore - KajDogaja? uporaba pri predavanjih iz podatkovnih baz (UP)
  - zbrati in dopolniti programsko podporo recenzijskega postopka pri odprti reviji Ars Mathematica Contemporanea (AMC) (izbira recenzentov, preverjanje jezika, plagiatorstva, ustreznosti ključnih besed, ustreznosti virov, itd.)
  - pregled stanja odprte znanosti v slovenskih knjižnicah; vključitev teh vsebin v izobraževanje knjižničarjev (UL FF)
  - pregled teoretičnih izhodišč in spletišč za poskuse s pristopom PRC (publish, review, curate); poskus navezave sodelovanja z izbranim(i).
  - podpora PRC  - avtomatski postopki pregleda: prepoznavanje avtorjev in povezava z ORCID, dodajanje DOI posameznim virom, ocena povezanosti virov s tematiko članka, ocena kakovosti posameznega vira, analiza povezanosti med avtorji in "ocenjevalci", prepoznavanje neetičnih dejavnosti, sorodni članki, ... poskusna uporaba v AMC, ... standardi ... OpenAlex, ArXiv, ...
  - podpora analitičnim postopkom podatkov iz OpenAlex; grafovske podatkovne baze (Neo4j, ...); vključitev teh vsebin v predmet analiza omrežij na študiju statistike (UL)
  - Goodhart: Formalni opis Goodhartovega zakona; Uporaba Goodhartovega zakona v informetriji; Razvoj metod za odkrivanje potecialno neetičnega vedenja pri znanstvenih objavah (avtorji, uredniki, recenzenti, založniki); Priprava priporočil za korekcijo stanja.
  - možne implicitne in eksplicitne mere kakovosti članka; Goodhart
  - nadaljnji razvoj metod, ki temeljijo na bibliometričnih omrežjih, in višjestopenjskih bibliografskih storitev 


Analize
  - Poskušamo oceniti hitrost sprememb na polju znanstvenga publiciranja, povečanje številk v zadnjih letih (št. objav, št. revij, APC + naročnin).
  - Razvoj FJN (Free Journal Network) Free journal network združuje nekatere revije diamantnega prostega dostopa.

Hekerski napadi

Pojav neetičnih objav in mehanizmi za njihovo izločanje.
  - Problem napačne klasifikacije revij in raziskovalcev. Na več mestih se pojavlja za neko enoto več kot ena klasifikacija področja raziskav. (npr. SICRIS, SCIMAGO). Zaradi različne kulture citiranja, lahko pride do hudih napak pri rangiranju znotraj posameznega področja. Izdelali bomo orodja za iskanje takšnih anomalij.
  - Problem razločevanja potencialno predatorskih metod pri revijah z visokim faktorjem vpliva. Iz samega faktorja vpliva ni mogoče določiti ali je revija primerna za objavo. Določili bomo metode, ki bodo s pomočjo drugih podatkov identificirale potencialno sporne revije.
  - Anketa ali več anket, s katerimi bomo dobili odziv znanstvenikov in drugih deležnikov na spremembe, ki jih prinaša odprta znanost.



**Predlogi raziskovalnih vprašanj v zvezi z odprto znanostjo:**

  - Kakšno je ujemanje med podatki o pobulikacijah (in OA) med OpenAlex in podatki v COBISS? Kako bi se lahko izboljšalo kakovost obeh virov? Kakšne vrste podatkov manjkajo?
  - Ali se delež objav v OA povečuje in kako hitro? Ali so pri tem razlike med različnimi raziskovalnimi področji in razisovalnimi organizacijami (glede na vrsto, velikost in druge karakteristike, če so na voljo)?
  - Kako se povečuje obseg različnih tipov OA (dimamantni, zlati, zeleni) in ali so pri tem kakšne razlike med razraziskovalnimi področji oz. organizacijami?
  - Kakšni so stroški objav v zlatem dostopu? Kako se stroški razlikujejo med različnimi raziskovalnimi področji in organizacijami? (Podatki o APC so na voljo prek spletnih strani založb, npr. za Elsevier na tej povezavi: https://assets.ctfassets.net/o78em1y1w4i4/5ZH2ckERsr69JYpnMue94T/d5bd104ae8483ff0b6f641c8dc3fb574/article-publishing-charge.xlsx)
  - Kako so različne vrste odprtega dostopa povezane s kakovostjo objav glede na izbrane kazalnike kakovosti?
  - Ali je možno objave v repozitorijih (v COBISS kategorija 2.20) povezati z objavami podatkov? Kakšen je delež objav, ki citirajo podatkovne zbirke podatkov?

### Predvideni rezultati

Pričakovani rezultati Goodhart
  - Članek na podlagi referata na IS 2024
  - Članek o statistično-teoretičnih osnovah Goodhartovega zakona
  - Prispevki za IS 2025,2026,2027
  - Članek o rezultatih anket
  - Članek o določanju potencialnih predatorjev.
  - Članek o izboljšavi klasificiranj
  - Prispevek za Dneve odprte znanosti 2026 in 2027 ter za konferenco Applied Statistics 2027
  - Objavljeni raziskovalni podatki v repozitoriju po načelih FAIR

### Detailed description of the work programme (23.3)

**WP1: Project management**

Task 1.1 Project administration and coordination

Management activities will take place during the entire project and will include administrative work related to work performance, quality assurance, risk assessment and mitigation, compliance with the proposal and contractual obligations with the Slovenian Research and Innovation Agency. In parallel, coordination activities will include fostering agreement on responsibilities, scheduling and coordinating team meetings and internal briefing on outcomes of individual activities.

Task 1.2 Data management

A data management plan (DMP) will be prepared in the first six months of the project (D1.1.) following the FAIR principles of making data findable, accessible, interoperable, and reusable. The DMP will detail the purpose of data collection, relevance to the objectives of the project, specify the types and formats of data and metadata to be collected, the expected size, and how data will be curated and preserved.

**WP2: Bibliographic data**

Task 2.1 Acquisition, cleaning and preparation bibliographic data (from OpenAlex and COBISS and administrative data from the University of Primorska and/or other institutions)?

Task 2.2 Developoment of knowledge graph

Task 2.3 Developoment of software for bibliographic analysis and services

Task 2.3 Analysis of bibliographic data

Task 2.5 Computation of measures of publication quality

Task 2.6 Detection of non-ethical publication practices

**WP3: PRC publication model  data**

Task 3.1 Acquisition, cleaning and preparation on data on PRC (i.e. data on APCs, editorial process, open peer reviews, etc.)

Task 3.2 Analysis of PRC data

**WP4 Survey data**

Task 3.1 Review of previous survey data on publishing

Task 3.2. Development of survey questionnaires

Task 3.3 Survey of researchers (at the University of Priomorska/in all Slovenian research institutions?)

Task 3.4 Survey of journal editors (for a sample of journals from all research field or for just one research field as a case study?)

Task 3.5 Analysis of suvey data

**WP5: Exploitation and dissemination of results?**

...
